{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtUO1uwz0eQ5xfIVD3pIKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sun9huni/first-repository/blob/main/0618_%EC%9B%8C%EB%93%9C_%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zi6Jh1_q75K7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeda0a06-2020-49b6-e8e8-c8bd9f8daca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqCaRIbfpr75",
        "outputId": "3673f47a-476f-43fd-c35c-639217dd471e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "YwDaPRthpsaY",
        "outputId": "f8d15645-85f8-46c9-87f7-84954bd9b6bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "b3ca90dc5f6a4314b4bc1d8460c492a1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "print(\"임포트 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVW4mrSIpu9I",
        "outputId": "d50d73aa-4100-4595-cb8f-ba7f6173191d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임포트 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.\"\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nyKPRrflqKeo",
        "outputId": "0c99b7c5-b650-4f58-8f7e-f74cb2868471"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 특수문자는 자연어 처리에서 큰 의미를 가지지 못한다. 정규 표현식을 사용해 특수문자를 제거한다.\n",
        "- 한글, 공백을 제외한 모든 문자를 표현하는 regex : [^ㄱ-ㅎㅏ-ㅣ가-힣 ]\n"
      ],
      "metadata": {
        "id": "NeOoKOxeqPTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg = re.compile(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\")\n",
        "text = reg.sub('', text)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sm280W_qfMp",
        "outputId": "f9d95ddd-965e-4927-b137-4878b8c99136"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임금님 귀는 당나귀 귀 임금님 귀는 당나귀 귀 실컷 소리치고 나니 속이 확 뚫려 살 것 같았어\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한국어는 주로 형태소 분석기를 통해서 토큰 단위를 나눠준다.\n",
        "- KoNLPy에 내장된 Okt 형태소 분석기를 사용"
      ],
      "metadata": {
        "id": "9-ckby9sqiyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "tokens = okt.morphs(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPjHVP-eqo4H",
        "outputId": "e0c93883-83f7-4639-dcf1-7b0cd8386845"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['임금님', '귀', '는', '당나귀', '귀', '임금님', '귀', '는', '당나귀', '귀', '실컷', '소리', '치고', '나니', '속이', '확', '뚫려', '살', '것', '같았어']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰을 바탕으로 단어장 만들기\n",
        "- 빈도수가 높은 단어일수록 낮은 정수 부여\n",
        "- 파이썬의 Counter 서브클래스를 사용해서 단어의 빈도를 카운트\n"
      ],
      "metadata": {
        "id": "RqWXn1fEqwNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter(tokens)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReQLgrOxq34B",
        "outputId": "83e21ff5-31ae-40ba-d03e-df5ae8762636"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'귀': 4, '임금님': 2, '는': 2, '당나귀': 2, '실컷': 1, '소리': 1, '치고': 1, '나니': 1, '속이': 1, '확': 1, '뚫려': 1, '살': 1, '것': 1, '같았어': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어가 key, 빈도수가 value로 지정"
      ],
      "metadata": {
        "id": "3bBNl5tNrCfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab['임금님']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjBZFtLtq-Ms",
        "outputId": "ab18738c-b249-4018-e6af-bb3075d5e73e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- most_common()는 상위 빈도수를 가진 단어를 주어진 수만큼 리컨한다.\n",
        "- 등장 빈도 수 상위 5개만 단어장으로 저장"
      ],
      "metadata": {
        "id": "yR_TSeKjrLIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trTDX-SGrVZl",
        "outputId": "57277145-0218-4f6f-f3bb-2722d6b65363"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('귀', 4), ('임금님', 2), ('는', 2), ('당나귀', 2), ('실컷', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여"
      ],
      "metadata": {
        "id": "bsymC4jUrmBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
        "print(word2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQGMESCKri0t",
        "outputId": "55e5db8f-0005-4408-b8b5-e90f52792e08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'귀': 1, '임금님': 2, '는': 3, '당나귀': 4, '실컷': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- one-hot encoding 함수를 만들어 각 단어를 one-hot vector로 변환"
      ],
      "metadata": {
        "id": "NzSTWJfdsLa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(word, word2index):\n",
        "  one_hot_vector = [0]*(len(word2index))\n",
        "  index = word2index[word]\n",
        "  one_hot_vector[index-1] = 1\n",
        "  return one_hot_vector\n",
        "print('슝=3')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uy8cq2osLEx",
        "outputId": "c96a6512-064c-4043-98b3-fa8f1c6660ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임금님 단어의 onehotvector\n",
        "one_hot_encoding(\"임금님\", word2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEg6P04KspsG",
        "outputId": "bd874cd7-5610-4723-d31b-c289bf4946d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스를 통한 One-hot-encoding\n",
        "Tokenizer와 to_categorical 사용"
      ],
      "metadata": {
        "id": "g5vub0ScspfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "print(\"임포트 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkI2ZJ70s6o-",
        "outputId": "71e29d19-be58-444d-c8c2-3cb9824a2132"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임포트 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [['강아지', '고양이', '강아지'],['애교', '고양이'], ['컴퓨터', '노트북']]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXbufUBotf7t",
        "outputId": "4494c5a1-eb3e-4e99-f112-4d8280ed5c12"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['강아지', '고양이', '강아지'], ['애교', '고양이'], ['컴퓨터', '노트북']]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- keras Tokenizer는 주어진 text로부터 Vocab을 만들고, 각 단어에 고유한 정수를 mapping"
      ],
      "metadata": {
        "id": "D1kOmKcwthZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(text)\n",
        "print(t.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9nSujEntoeB",
        "outputId": "9f0f867e-2352-4275-eff8-759233ef2157"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'강아지': 1, '고양이': 2, '애교': 3, '컴퓨터': 4, '노트북': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어장의 크기를 voacb_size에 저장\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FULuWHuLtyko",
        "outputId": "0a2e7aaa-f9f1-4f05-c87a-85a88a727a11"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어장에 속한 단어들로 구성된 텍스트 시퀀스는 케라스 토크나이저를 통해 정수 시퀀스로 변환가능\n",
        "sub_text = ['강아지', '고양이', '강아지', '컴퓨터']\n",
        "encoded = t.texts_to_sequences([sub_text])\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbwKznNyt6wl",
        "outputId": "710db46b-d5a1-4daa-9262-bc346937a759"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 1, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이렇게 변환된 정수 시퀀스는 to_categorical()을 사용해 원-핫 벡터의 시퀀스로 변환 가능\n",
        "one_hot = to_categorical(encoded, num_classes = vocab_size)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h78hIqpsuD-b",
        "outputId": "51b91deb-dffc-4252-cdbf-b38c9b767bbe"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec\n",
        "- Word2Vec은 은닉층이 1개라서 딥 러닝이라기보다는 얕은 신경망(Shallow Neural Network) 을 학습\n",
        "\n",
        "(1) 분포가설\n",
        "- 분포 가설은 비슷한 문맥에서 같이 등장하는 경향이 있는 단어들은 비슷한 의미를 가진다는 가설이다.\n",
        "\n",
        "(2) CBoW(Continuous Bag of words)\n",
        "- 주변에 있는 단어들을 통해 중간에 있는 단어들을 예측하는 방법\n",
        "- 단어의 범위를 window라고 한다.\n",
        "- 윈도우 크기를 정한 후 원도우를 계속 움직이면서 학습 위한 데이터셋을 만드는 방법이 sliding window\n",
        "- 이렇게 선택된 데이터셋에서 단어 각각은 원-핫 인코딩되어 원-핫 벡터가 되고, 원-핫 벡터가 CBoW나 Skip-gram의 입력이 된다.\n",
        "- 원-핫 벡터와 가중치 행렬과의 곱은 가중치 행렬의 i 위치에 있는 행을 그대로 가져오는 것과 동일\n",
        "- 이를  룩업 테이블(lookup table)이라고 한다.\n",
        "- 룩업 데이블을 거쳐 생긴 벡터들을 모두 합하거나, 평균을 구한 값을 최종 은닉층의 결과로 한다.\n",
        "- Word2Vec 은닉층은 활성화 함수가 존재하지 않고, 단순 가중치 행렬과의 곱셈만을 수행하기에 투사층(projection layer)라고 한다.\n",
        "- 은닉층에서 생성된 N차원의 벡터는 두 번째 가중치 행렬과 곱해진다. 이 가중치 행렬의 크기는 (N x V), 곱셈의 결과로 나오는 벡터의 차원은 V이다.\n",
        "- 출력층은 softmax를 사용하므로 V 차원의 벡터는 활성화 함수를 거쳐 모든 총합이 1이 되는 벡터로 변경된다.\n",
        "- CBoW는 이 출력층의 벡터를 중심 단어의 원-핫 벡터와의 손실(loss)을 최소화 하도록 학습\n",
        "\n",
        "(3) Skip-Gram\n",
        "- 중간에 있는 단어로 주변 단어를 예측하는 방법\n",
        "- 은닉층에서 다수의 벡터의 덧셈과 평균을 구하는 과정이 없다.\n",
        "- 네거티브 샘플링(Negative Sampling)을 사용한다.\n",
        "- 네거티브 샘플링은 연산량을 줄이기 위해 다중 클래스 분류 문제를 시그모이드 함수를 이용한 이진 분류 문제로 바꾼다.\n",
        "- 랜덤으로 단어장에 있는 아무 단어나 가져와 target word로 하는 거짓 데이터셋을 만들고 0으로 레이블링을 해주는 것"
      ],
      "metadata": {
        "id": "1LGIjpuCwk65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec (4) 영어 Word2Vec 실습과 OOV 문제"
      ],
      "metadata": {
        "id": "XATJ1B1cwkyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('abc')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsaSNInY7Mn9",
        "outputId": "96df57b1-7b5e-4a53-c813-95fe340de343"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/abc.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import abc\n",
        "corpus = abc.sents()\n",
        "print(\"슝~\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRjtFw_v7Wdo",
        "outputId": "380bb924-a799-4c20-ca68-f82df799cace"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQAIbD-V7aMr",
        "outputId": "606ff9e4-fda9-4b7c-ddc6-247e71ff0a6a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.'], ['Letters', 'from', 'John', 'Howard', 'and', 'Deputy', 'Prime', 'Minister', 'Mark', 'Vaile', 'to', 'AWB', 'have', 'been', 'released', 'by', 'the', 'Cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ['In', 'one', 'of', 'the', 'letters', 'Mr', 'Howard', 'asks', 'AWB', 'managing', 'director', 'Andrew', 'Lindberg', 'to', 'remain', 'in', 'close', 'contact', 'with', 'the', 'Government', 'on', 'Iraq', 'wheat', 'sales', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import abc\n",
        "corpus = list(abc.sents())\n",
        "corpus_length = len(corpus)\n",
        "print('코퍼스의 크기 :', corpus_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw8Xf2jF7erZ",
        "outputId": "fa44e2b3-b91f-4d2b-9c98-c61a6cb8c23a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "코퍼스의 크기 : 29059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences = corpus, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\n",
        "print('모델 학습 완료')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uve1PLle72jA",
        "outputId": "b1c35012-eda4-4cba-ed04-d2b22070e180"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 학습 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- vector size = 학습 후 Embedding vector의 차원\n",
        "- window = Context Window 크기\n",
        "- min_count = 단어 최소 빈도수 제한 (빈도가 적은 단어들은 학습하지 않아요.)\n",
        "- workers = 학습을 위한 프로세스 수\n",
        "- sg = 0은 CBoW, 1은 Skip-gram."
      ],
      "metadata": {
        "id": "O6boiLHH8Ifa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec는 입력한 단어에 대해서 가장 코사인 유사도가 높은 단어들을 출력하는 model.wv.most_similar를 지원\n",
        "model_result = model.wv.most_similar('man')\n",
        "print(model_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSHd5a3g8MvU",
        "outputId": "72155884-7b97-4d23-cf0a-ef114fc23c6c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('skull', 0.9349513649940491), ('third', 0.9185877442359924), ('woman', 0.9181984066963196), ('ten', 0.8983795642852783), ('star', 0.8982238173484802), ('fifth', 0.8978276252746582), ('field', 0.8941973447799683), ('galaxy', 0.8939309120178223), ('dog', 0.8935268521308899), ('tunnel', 0.8911997675895691)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장 및 로드\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model.wv.save_word2vec_format('./w2v')\n",
        "loaded_model = KeyedVectors.load_word2vec_format(\"./w2v\")\n",
        "print(\"모델  load 완료!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbdcGPkq8c7w",
        "outputId": "d59ed543-c04c-4956-a3dd-bea694532e1d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델  load 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 로드한 모델이 이전과 동일한 결과를 출력하는지 테스트\n",
        "model_result = loaded_model.most_similar(\"man\")\n",
        "print(model_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOmOs8bG8wtH",
        "outputId": "e88fb4bd-676a-4e51-fdb4-1c5aa6bacde1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('skull', 0.9349513649940491), ('third', 0.9185877442359924), ('woman', 0.9181984066963196), ('ten', 0.8983795642852783), ('star', 0.8982238173484802), ('fifth', 0.8978276252746582), ('field', 0.8941973447799683), ('galaxy', 0.8939309120178223), ('dog', 0.8935268521308899), ('tunnel', 0.8911997675895691)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 에러가 나더라도 놀라지 마세요.\n",
        "loaded_model.most_similar('overacting')\n",
        "## OOV 문제"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "aopWfQxG85fW",
        "outputId": "124fa20c-2258-4ffe-a52c-bc25673e345a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'overacting' not present in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-666610142>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 에러가 나더라도 놀라지 마세요.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overacting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'overacting' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.most_similar('memorry')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "Fjwc9L3z-E0U",
        "outputId": "afd6c4e7-a051-42ae-9b41-c8ca92cb17c5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'memorry' not present in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3442707555>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memorry'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'memorry' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17-9. 임베딩 벡터의 시각화\n",
        "- 임베딩 프로젝터(embedding projector) 를 사용해서 임베딩 벡터들을 시각화\n",
        "- 어떤 임베팅 벡터들이 가까운 거리에 군집이 되어 있고, 특정 임베딩 벡터와 유클리드 거리나 코사인 유사도가 높은지 확인 가능\n",
        "\n"
      ],
      "metadata": {
        "id": "fnEsNw5K-OK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17-10. FastText\n",
        "- 매커니즘 자체는 Word2Vec을 따르지만, 문자 단위 n-gram(character-level n-gram) 표현을 학습한다\n",
        "- FastText는 단어 내부의 내부 단어(subwords)들을 학습한다는 아이디어\n",
        "- n은 단어들이 얼마나 분리되는지 결정하는 하이퍼파라미터\n",
        "- 시작과 끝을 의미하는 <, >를 도입\n",
        "- <pa, par, art, rti, tia, ial, al>라는 6개의 내부 단어(subword) 토큰을 벡터로 만든다.\n",
        "- 여기에 추가적으로 기존 단어에 <, 와 >를 붙인 토큰 <partial>을 벡터화한다.\n",
        "- 이렇게 벡터화된 n-gram 벡터들의 총합을 해당 단어의 벡터로 취한다.\n",
        "- FastText도 Word2Vec과 마찬가지로 네거티브 샘플링을 사용하여 학습한다.\n",
        "- Word2Vec과 다른 점은 학습 과정에서 중심 단어에 속한 문자 단위 n-gram 단어 벡터들을 모두 업데이트한다는 점\n",
        "- FastText는 Word2Vec과 달리 OOV와 오타에 강건하다(robust) 는 특징\n",
        "- 단어장에 없는 단어라도, 해당 단어의 n-gram이 다른 단어에 존재하면 이로부터 벡터값을 얻는다는 원리에 기인"
      ],
      "metadata": {
        "id": "vI7bFd6A_PRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vec 모델 메타정보 및 텐서 내보내기\n",
        "!python -m gensim.scripts.word2vec2tensor --input ./w2v --output ./w2v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAbu_jdh-NjA",
        "outputId": "2d3e2e0b-1353-442b-afb6-40bb48c909e9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-18 07:40:53,776 - word2vec2tensor - INFO - running /usr/local/lib/python3.11/dist-packages/gensim/scripts/word2vec2tensor.py --input ./w2v --output ./w2v\n",
            "2025-06-18 07:40:53,776 - keyedvectors - INFO - loading projection weights from ./w2v\n",
            "2025-06-18 07:40:54,489 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (10363, 100) matrix of type float32 from ./w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-06-18T07:40:54.486607', 'gensim': '4.3.3', 'python': '3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]', 'platform': 'Linux-6.1.123+-x86_64-with-glibc2.35', 'event': 'load_word2vec_format'}\n",
            "2025-06-18 07:40:55,249 - word2vec2tensor - INFO - 2D tensor file saved to ./w2v_tensor.tsv\n",
            "2025-06-18 07:40:55,249 - word2vec2tensor - INFO - Tensor metadata file saved to ./w2v_metadata.tsv\n",
            "2025-06-18 07:40:55,249 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "fasttext_model = FastText(corpus, window=5, min_count=5, workers=4, sg=1)\n",
        "print(\"FastText 학습 완료!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GtVcJEOAV93",
        "outputId": "18250531-a474-4f7b-b2be-98f9d2e22f7f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText 학습 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model.wv.most_similar('overacting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Uga9e5AYot",
        "outputId": "a7d3312e-a4c4-49c5-9e53-657bceed4d69"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('extracting', 0.9463399052619934),\n",
              " ('lifting', 0.9393213987350464),\n",
              " ('losing', 0.9376756548881531),\n",
              " ('blushing', 0.9345923662185669),\n",
              " ('fluctuating', 0.9313043355941772),\n",
              " ('mixing', 0.9310101866722107),\n",
              " ('attracting', 0.9308171272277832),\n",
              " ('shifting', 0.9307165145874023),\n",
              " ('emptying', 0.9299069046974182),\n",
              " ('rotating', 0.9279723167419434)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model.wv.most_similar('memoryy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VApkiksTAZ5d",
        "outputId": "a63902c1-2175-4bd8-c76d-7d6e77f382dd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('memory', 0.9513688087463379),\n",
              " ('musical', 0.8755364418029785),\n",
              " ('intelligence', 0.8468678593635559),\n",
              " ('mechanical', 0.8463777303695679),\n",
              " ('emotion', 0.8421338796615601),\n",
              " ('visual', 0.8412631154060364),\n",
              " ('actual', 0.8411545753479004),\n",
              " ('experience', 0.8390754461288452),\n",
              " ('mechanism', 0.8361741304397583),\n",
              " ('cognitive', 0.8358390927314758)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 'overacting'이 단어장에 없던 단어임에도 정상적으로 임베딩 벡터값이 계산되어 유사 단어 10개를 출력하는 것을 볼 수 있다. 이는 오타를 가정한 단어 'memoryy'에서도 마찬가지이다."
      ],
      "metadata": {
        "id": "IvhGHfmaAgN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17-11. GloVe\n",
        "글로브(Global Vectors for Word Representation, GloVe)는 카운트 기반과 예측 기반 두 가지 방법을 모두 사용\n",
        "\n",
        "- DTM의 경우에는 단어 간 유사도를 반영할 수 없고, 대부분의 값이 0인 희소 표현이라는 특징, DTM을 차원 축소하여 밀집 표현(dense representation)으로 임베딩 하는 방법이 LSA(Latent Semantic Analysis)\n",
        "- LSA는 DTM에 특잇값 분해를 사용해 잠재적 의미를 이끌어내는 방법론이다.\n",
        "\n",
        "LSA 같은 카운트 기반 임베딩 방법의 한계점\n",
        "- 차원 축소의 특성으로 인해 새로운 단어 추가 시 다시 DTM을 만들어 새로 차원 축소를 해야 한다.\n",
        "- 단어 벡터 간 유사도를 계산하는 측면에서 Word2Vec보다 성능이 떨어진다.\n",
        "\n",
        "예측 기반의 방법 은 Word2Vec과 같은 방법\n",
        "- 인공 신경망이 예측한 값으로부터 실제 레이블과의 오차를 구하고, 손실 함수를 통해서 인공 신경망을 학습하는 방식\n",
        "- LSA보다 단어 벡터 간 유사도를 구하는 능력은 뛰어나지만, LSA처럼 corpus의 전체적인 통계 정보를 활용하지는 못한다는 점이 한계\n"
      ],
      "metadata": {
        "id": "U1l1dOWoA6yZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)\n",
        "- 행과 열을 전체 vocab의 단어들로 구성하고, 어떤 i 단어의 window Size 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬\n",
        "- 전치(transpose)해도 동일한 행렬이 된다는 특징\n",
        "\n",
        "### 동시 등장 확률(Co-occurrence Probability)\n",
        "- 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률\n",
        "- i를 중심 단어(center word), k를 주변 단어(context word)라고 한다.\n",
        "\n",
        "### GloVe의 손실 함수 설계하기\n",
        "- 동시 등장 행렬(카운트 기반)의 동시 등장 확률을 이용해 손실 함수(예측 기반)를 설계\n",
        "- GloVe는 전체 corpus에서의 동시 등장 빈도의 로그값과 중심 단어 벡터와 주변 단어 벡터의 내적값의 차이가 최소화되도록 두 벡터의 값을 학습하는 것\n",
        "- GloVe의 연구진은 동시 등장 행렬에서 동시 등장 빈도의 값이 굉장히 낮은 경우에는 거의 도움이 되지 않는 정보라고 판단했습니다. 그래서 이에 대한 가중치를 주기 위해서 GloVe 연구진이 선택한 것은 바로\n",
        "동시 등장 빈도의 값에 영향을 받는 가중치 함수(Weighting function)를 도입\n",
        "\n"
      ],
      "metadata": {
        "id": "Q4OjRPUYCV3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "glove_model = api.load(\"glove-wiki-gigaword-50\")  # glove vectors 다운로드\n",
        "glove_model.most_similar(\"dog\")  # 'dog'과 비슷한 단어 찾기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaukPlyvDg23",
        "outputId": "acd82047-d5f4-483e-aaf3-087e3cacf9b8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat', 0.9218004941940308),\n",
              " ('dogs', 0.8513158559799194),\n",
              " ('horse', 0.7907583713531494),\n",
              " ('puppy', 0.7754920721054077),\n",
              " ('pet', 0.7724708318710327),\n",
              " ('rabbit', 0.7720814347267151),\n",
              " ('pig', 0.7490062117576599),\n",
              " ('snake', 0.7399188876152039),\n",
              " ('baby', 0.7395570278167725),\n",
              " ('bite', 0.7387937307357788)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model.most_similar('overacting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFfIEgvXDiQe",
        "outputId": "c51ac61b-3308-4539-dad6-cad81510a30e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('impudence', 0.7842012047767639),\n",
              " ('puerile', 0.7816032767295837),\n",
              " ('winningly', 0.7644237875938416),\n",
              " ('grossness', 0.7576098442077637),\n",
              " ('deconstructions', 0.748936653137207),\n",
              " ('over-the-top', 0.7460805773735046),\n",
              " ('buffoonery', 0.746045708656311),\n",
              " ('impetuosity', 0.7415392398834229),\n",
              " ('sophomoric', 0.736961841583252),\n",
              " ('zaniness', 0.7353197336196899)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model.most_similar('memoryy')\n",
        "# GloVe는 Word2Vec과 같이 OOV 문제를 가지고 있어서 'memoryy'라는 단어는 인식하지 못합니다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "vYnBCvlTDmzO",
        "outputId": "e1df0a8d-7847-45d4-a643-4c6bc56e8057"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'memoryy' not present in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3329734643>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memoryy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# GloVe는 Word2Vec과 같이 OOV 문제를 가지고 있어서 'memoryy'라는 단어는 인식하지 못합니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'memoryy' not present in vocabulary\""
          ]
        }
      ]
    }
  ]
}