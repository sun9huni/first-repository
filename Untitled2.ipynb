{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c814fb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.3\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import tensorflow\n",
    "\n",
    "print(pandas.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8792e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6acb8075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>3186</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-12-31 22:00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1270</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-12-31 22:00:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1721</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-12-31 22:00:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1022</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-12-31 22:00:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2340</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:01:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000019</th>\n",
       "      <td>6040</td>\n",
       "      <td>2917</td>\n",
       "      <td>4</td>\n",
       "      <td>2001-08-10 14:40:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999988</th>\n",
       "      <td>6040</td>\n",
       "      <td>1921</td>\n",
       "      <td>4</td>\n",
       "      <td>2001-08-10 14:41:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000172</th>\n",
       "      <td>6040</td>\n",
       "      <td>1784</td>\n",
       "      <td>3</td>\n",
       "      <td>2001-08-10 14:41:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000167</th>\n",
       "      <td>6040</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>2001-08-10 14:41:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000042</th>\n",
       "      <td>6040</td>\n",
       "      <td>1221</td>\n",
       "      <td>4</td>\n",
       "      <td>2001-08-20 13:44:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserId  ItemId  Rating                Time\n",
       "31            1    3186       4 2000-12-31 22:00:19\n",
       "22            1    1270       5 2000-12-31 22:00:55\n",
       "27            1    1721       4 2000-12-31 22:00:55\n",
       "37            1    1022       5 2000-12-31 22:00:55\n",
       "24            1    2340       3 2000-12-31 22:01:43\n",
       "...         ...     ...     ...                 ...\n",
       "1000019    6040    2917       4 2001-08-10 14:40:29\n",
       "999988     6040    1921       4 2001-08-10 14:41:04\n",
       "1000172    6040    1784       3 2001-08-10 14:41:04\n",
       "1000167    6040     161       3 2001-08-10 14:41:26\n",
       "1000042    6040    1221       4 2001-08-20 13:44:15\n",
       "\n",
       "[1000209 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(os.getenv('HOME')+'/aiffel/yoochoose/data/') \n",
    "train_path = data_path / 'ratings.dat'\n",
    "\n",
    "def load_data(data_path: Path, nrows=None):\n",
    "    data = pd.read_csv(data_path, sep='::', header=None, usecols=[0, 1, 2, 3], dtype={0: np.int32, 1: np.int32, 2: np.int32}, nrows=nrows)\n",
    "    data.columns = ['UserId', 'ItemId', 'Rating', 'Time']\n",
    "    data['Time'] = pd.to_datetime(data['Time'], unit='s')\n",
    "    return data\n",
    "\n",
    "data = load_data(train_path, None)\n",
    "data.sort_values(['UserId', 'Time'], inplace=True)  # data를 id와 시간 순서로 정렬해줍니다.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "995a069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_recursive(data: pd.DataFrame, shortest, least_click) -> pd.DataFrame:\n",
    "    while True:\n",
    "        before_len = len(data)\n",
    "        data = cleanse_short_session(data, shortest)\n",
    "        data = cleanse_unpopular_item(data, least_click)\n",
    "        after_len = len(data)\n",
    "        if before_len == after_len:\n",
    "            break\n",
    "    return data\n",
    "\n",
    "\n",
    "def cleanse_short_session(data: pd.DataFrame, shortest):\n",
    "    session_len = data.groupby('UserId').size()\n",
    "    session_use = session_len[session_len >= shortest].index\n",
    "    data = data[data['UserId'].isin(session_use)]\n",
    "    return data\n",
    "\n",
    "\n",
    "def cleanse_unpopular_item(data: pd.DataFrame, least_click):\n",
    "    item_popular = data.groupby('ItemId').size()\n",
    "    item_use = item_popular[item_popular >= least_click].index\n",
    "    data = data[data['ItemId'].isin(item_use)]\n",
    "    return data\n",
    "\n",
    "data = cleanse_recursive(data, shortest=2, least_click=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98373362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_date(data: pd.DataFrame, n_days: int):\n",
    "    final_time = data['Time'].max()\n",
    "    cutoff_time = final_time - dt.timedelta(n_days)\n",
    "    \n",
    "    train = data[data['Time'] < cutoff_time]\n",
    "    test = data[data['Time'] >= cutoff_time]\n",
    "    return train, test\n",
    "\n",
    "tr, test = split_by_date(data, n_days=1)\n",
    "tr, val = split_by_date(tr, n_days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f0ef642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 데이터의 아이템을 기준으로 인덱스 사전 생성\n",
    "id2idx = {item_id : index for index, item_id in enumerate(tr['ItemId'].unique())}\n",
    "\n",
    "def indexing(df, id2idx):\n",
    "    # Train 데이터에 없는 아이템은 -1로 처리 (나중에 무시됨)\n",
    "    df['item_idx'] = df['ItemId'].map(lambda x: id2idx.get(x, -1))\n",
    "    return df\n",
    "\n",
    "tr = indexing(tr, id2idx)\n",
    "val = indexing(val, id2idx)\n",
    "test = indexing(test, id2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3607e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    def __init__(self, data):\n",
    "        self.df = data\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx = np.arange(self.df['UserId'].nunique())\n",
    "\n",
    "    def get_click_offsets(self):\n",
    "        offsets = np.zeros(self.df['UserId'].nunique() + 1, dtype=np.int32)\n",
    "        offsets[1:] = self.df.groupby('UserId').size().cumsum()\n",
    "        return offsets\n",
    "\n",
    "class FullSequenceDataLoader:\n",
    "    def __init__(self, dataset: SessionDataset, batch_size=16):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.sessions = self._build_sessions()\n",
    "\n",
    "    def _build_sessions(self):\n",
    "        sessions = []\n",
    "        for start, end in zip(self.dataset.click_offsets[:-1], self.dataset.click_offsets[1:]):\n",
    "            session = self.dataset.df['item_idx'].values[start:end]\n",
    "            if len(session) >= 2:\n",
    "                sessions.append(session)\n",
    "        return sessions\n",
    "\n",
    "    # 'for' 루프를 가능하게 하는 __iter__ 메서드\n",
    "    def __iter__(self):\n",
    "        # 세션 리스트를 배치 사이즈만큼 순회\n",
    "        for i in range(0, len(self.sessions), self.batch_size):\n",
    "            batch = self.sessions[i:i+self.batch_size]\n",
    "            input_batch = []\n",
    "            target_batch = []\n",
    "\n",
    "            for session in batch:\n",
    "                input_batch.append(session[:-1])    # 예: [A, B, C]\n",
    "                target_batch.append(session[-1])    # 예: D\n",
    "            \n",
    "            # 처리된 배치 데이터를 반환\n",
    "            yield input_batch, target_batch\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_dataset = SessionDataset(tr)\n",
    "train_loader = FullSequenceDataLoader(train_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb392e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None, 3416)]      0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 50)                520200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3416)              174216    \n",
      "=================================================================\n",
      "Total params: 694,416\n",
      "Trainable params: 694,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "def create_model(args):\n",
    "    inputs = Input(shape=(None, args.num_items))  # (배치 크기, 시퀀스 길이, 전체 아이템 수)\n",
    "    x = GRU(args.hsz, return_sequences=False)(inputs)\n",
    "    x = Dropout(args.drop_rate)(x)\n",
    "    predictions = Dense(args.num_items, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(args.lr), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "class Args:\n",
    "    # class 선언 아래에 4칸 들여쓰기\n",
    "    def __init__(self, tr, val, test, batch_size, hsz, drop_rate, lr, epochs, k):\n",
    "        self.tr = tr\n",
    "        self.val = val\n",
    "        self.test = test\n",
    "        self.num_items = tr['ItemId'].nunique()\n",
    "        self.num_sessions = tr['UserId'].nunique()\n",
    "        self.batch_size = batch_size\n",
    "        self.hsz = hsz\n",
    "        self.drop_rate = drop_rate\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.k = k\n",
    "\n",
    "args = Args(tr, val, test, batch_size=16, hsz=50, drop_rate=0.1, lr=0.001, epochs=3, k=20)\n",
    "\n",
    "model = create_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "082797a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 378it [03:37,  1.74it/s, accuracy=0, train_loss=6.49]     \n",
      "Evaluation: 1it [00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Recall@20 epoch 1: 0.0000\n",
      "\t - MRR@20    epoch 1: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 378it [03:37,  1.74it/s, accuracy=0, train_loss=6.13]     \n",
      "Evaluation: 1it [00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Recall@20 epoch 2: 0.0000\n",
      "\t - MRR@20    epoch 2: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 378it [03:37,  1.74it/s, accuracy=0.125, train_loss=5.97] \n",
      "Evaluation: 1it [00:00, 14.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Recall@20 epoch 3: 0.0000\n",
      "\t - MRR@20    epoch 3: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def train_model(model, args):\n",
    "    train_dataset = SessionDataset(args.tr)\n",
    "    train_loader = FullSequenceDataLoader(train_dataset, batch_size=args.batch_size)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        tr_loader = tqdm(train_loader, desc=f'Train Epoch {epoch}')\n",
    "\n",
    "        for input_seqs, target_items in tr_loader:\n",
    "            # 1. 시퀀스 길이를 맞추기 위해 패딩(padding)을 적용합니다.\n",
    "            input_seqs_padded = pad_sequences(input_seqs, padding='pre', maxlen=None)\n",
    "            \n",
    "            # 2. 패딩된 시퀀스를 One-hot-encoding 합니다.\n",
    "            input_ohe = to_categorical(input_seqs_padded, num_classes=args.num_items)\n",
    "            \n",
    "            # 3. 정답(target) 아이템을 One-hot-encoding 합니다.\n",
    "            target_ohe = to_categorical(target_items, num_classes=args.num_items)\n",
    "\n",
    "            # 이제 변수가 정의되었으므로 오류 없이 실행됩니다.\n",
    "            result = model.train_on_batch(input_ohe, target_ohe)\n",
    "            tr_loader.set_postfix(train_loss=result[0], accuracy=result[1])\n",
    "        \n",
    "        # 각 에포크 종료 후 검증 데이터로 성능 평가\n",
    "        val_recall, val_mrr = get_metrics(args.val, model, args, args.k)\n",
    "        print(f\"\\n\\t - Recall@{args.k} epoch {epoch}: {val_recall:.4f}\")\n",
    "        print(f\"\\t - MRR@{args.k}    epoch {epoch}: {val_mrr:.4f}\\n\")\n",
    "        \n",
    "def get_metrics(data, model, args, k: int):\n",
    "    dataset = SessionDataset(data)\n",
    "    loader = FullSequenceDataLoader(dataset, batch_size=args.batch_size)\n",
    "\n",
    "    recall_list, mrr_list = [], []\n",
    "\n",
    "    for input_seqs, label in tqdm(loader, desc='Evaluation'):\n",
    "        input_seqs_padded = pad_sequences(input_seqs, padding='pre', maxlen=100)\n",
    "        input_ohe = to_categorical(input_seqs_padded, num_classes=args.num_items)  # (B, T, num_items)\n",
    "\n",
    "        pred = model.predict(input_ohe, batch_size=args.batch_size)  # (B, num_items)\n",
    "        pred_arg = tf.argsort(pred, direction='DESCENDING')\n",
    "\n",
    "        for i in range(len(label)):\n",
    "            recall_list.append(recall_k(pred_arg[i], label[i], k))\n",
    "            mrr_list.append(mrr_k(pred_arg[i], label[i], k))\n",
    "\n",
    "    return np.mean(recall_list), np.mean(mrr_list)\n",
    "\n",
    "def recall_k(pred, truth: int, k: int) -> int:\n",
    "    answer = truth in pred[:k]\n",
    "    return int(answer)\n",
    "\n",
    "def mrr_k(pred, truth: int, k: int):\n",
    "    indexing = np.where(pred[:k] == truth)[0]\n",
    "    if len(indexing) > 0:\n",
    "        return 1 / (indexing[0] + 1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 모델 학습 시작\n",
    "train_model(model, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
